
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Research Paper</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif;
      margin: 2rem auto;
      max-width: 800px;
      padding: 0 1rem;
      line-height: 1.6;
      color: #333;
      background-color: #fff;
    }
    h2.paper-title {
      font-size: 1.8em;
      font-weight: 700;
      text-align: center;
      margin-bottom: 0.5em;
      border-bottom: none;
    }
    h2 {
      border-bottom: 2px solid #ddd;
      padding-bottom: 0.3em;
      margin-top: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      overflow: auto;
      border-radius: 5px;
    }
    code {
      font-family: Menlo, Monaco, Consolas, monospace;
    }
    ul {
      padding-left: 1.5em;
    }
    figure {
      text-align: center;
      margin: 1.5em 0;
      background: none !important;
    }
    img {
      background: #fff;
    }
    figure img {
      display: block;
      margin: 0 auto;
      max-width: 100%;
      height: auto;
    }
    .img-pair .pair {
      display: flex;
      justify-content: space-between;
    }
    .img-pair img {
      max-width: 48%;
      height: auto;
    }
    figcaption {
      font-size: 0.9em;
      color: #666;
    }
  </style>
</head>
<body>
<h2 class="paper-title">BLADE: A 64-Byte Layer-Wise 1-Bit Gradient Fingerprint for Stable LLM Fine-Tuning</h2>

<section>
  <h2>Abstract</h2>
  <p>Fine-tuning large language models on small mathematical-reasoning corpora is notoriously unstable: tiny learning-rate errors often drive the loss to NaN, and even successful runs show large seed-to-seed variance. Existing controllers act on a single global scalar and rely on kilobytes to megabytes of auxiliary state, a poor fit for single-GPU or edge deployment. We propose BLADE, a Binarised LAyer-wise Direction Estimator that requires only 64 bytes for a 32-layer transformer. For each layer BLADE stores the previous step’s gradient signs at 16 randomly chosen weights, compares them to the current signs with one XOR-popcount, and derives a binary alignment score. This signal is fused with normalised loss, entropy and gradient norm into a per-layer learning-rate multiplier; a global median and conservative clamping ensure stability, while a periodic reservoir refresh prevents staleness. Implemented as a PyTorch hook, BLADE adds less than 0.05 ms per iteration and no extra VRAM. We integrate BLADE into AdamW and fine-tune Qwen3-0.6B on GSM8K, benchmarking against the model-level SKETCH-ALIGN controller. Although both configurations diverged in the present iteration, BLADE incurred negligible overhead and surfaced early-instability patterns that global controllers overlook. All code, logs and figures are released to catalyse follow-up stabilisation and broader evaluation.</p>
</section>

<section>
  <h2>Introduction</h2>
  <p>Large language models (LLMs) have recently displayed impressive in-context mathematical reasoning, but transferring that competence to small, high-precision datasets such as GSM8K still demands parameter fine-tuning. In practice, success hinges on a fragile equilibrium between the global learning-rate (LR) schedule, layer-wise curvature and stochastic gradient noise. A slight mis-configuration may trigger exploding activations, underflowing gradients or silent numerical instabilities that culminate in NaNs. Such sensitivity hampers reproducibility for researchers without the compute budget for exhaustive hyper-parameter sweeps and undermines edge deployments where every millisecond and megabyte matter.</p>
  <p>Prior work attacks LR selection from several angles. AutoLRS performs on-the-fly Bayesian optimisation over schedule segments <a href="https://arxiv.org/pdf/2105.10762v1.pdf" target="_blank" title="AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly">(Yuchen Jin, 2021)</a>; MoMo models local curvature to adjust Polyak-style rates <a href="https://arxiv.org/pdf/2305.07583v3.pdf" target="_blank" title="MoMo: Momentum Models for Adaptive Learning Rates">(Fabian Schaipp, 2023)</a>; and AdaScale scales LR in proportion to gradient variance, enabling large-batch training without quality loss <a href="https://arxiv.org/pdf/2007.05105v1.pdf" target="_blank" title="AdaScale SGD: A User-Friendly Algorithm for Distributed Training">(Tyler B. Johnson, 2020)</a>. Theoretical studies illuminate optimal schedules under distribution shift <a href="https://arxiv.org/pdf/2303.15634v2.pdf" target="_blank" title="Learning Rate Schedules in the Presence of Distribution Shift">(Matthew Fahrbach, 2023)</a> and compute constraints <a href="https://arxiv.org/pdf/2405.18392v3.pdf" target="_blank" title="Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations">(Alexander Hägele, 2024)</a>, while practical tools such as Mechanic wrap popular heuristics behind a user-friendly interface <a href="#ref-author-year-mechanic" target="_blank" title="Mechanic: A Learning Rate Tuner">(author-year-mechanic)</a>. All of these methods, however, treat the model as a monolith and therefore miss heterogeneous layer dynamics.</p>
  <p>Why is model-level control insufficient? Transformer blocks often experience widely different curvature and noise levels. Suppose attention layers oscillate because their gradients flip sign every other step while neighbouring MLP layers follow a stable trajectory. A scalar LR multiplier is blind to this heterogeneity: damping oscillating layers slows down already stable ones, whereas accelerating stable layers exacerbates oscillations. Ideally, the optimiser would sense discord at layer granularity and react quickly without paying a steep memory or compute tax.</p>
  <p>We introduce BLADE, a byte-sized controller that fulfils these desiderata. The key insight is that a single bit per parameter suffices to detect directional coherence: with only 16 bits per layer we can reliably flag misaligned updates. Combining this binary signal with inexpensive scalars—loss, output entropy and gradient norm—yields a robust estimator of how aggressive or conservative each layer’s step should be. All operations are integer (XOR, popcount, comparisons) and the total buffer is four machine words, making BLADE a natural fit for single-GPU fine-tuning loops.</p>
  <ul>
    <li><strong>64-byte, 1-bit layer-wise fingerprint:</strong> We devise a 64-byte, 1-bit layer-wise fingerprint that captures gradient alignment with purely bitwise operations.</li>
    <li><strong>Quad-signal multiplier:</strong> We formulate a quad-signal multiplier that blends loss, entropy, gradient norm and alignment through smoothed ratios, a global median and safe clamping.</li>
    <li><strong>Periodic reservoir update:</strong> We show how a periodic reservoir update refreshes the fingerprint at negligible cost, preventing staleness.</li>
    <li><strong>PyTorch hook:</strong> We implement BLADE as a PyTorch hook and release full artefacts for single-GPU fine-tuning of Qwen3-0.6B on GSM8K.</li>
    <li><strong>Initial comparison against SKETCH-ALIGN:</strong> We conduct an initial comparison against the SKETCH-ALIGN controller. Although both runs diverged, the setup exposes early-instability failure modes and quantifies BLADE’s overhead (&lt;0.1 % runtime, 64 B state), paving the way for stabilisation and extended studies.</li>
  </ul>
  <p>The remainder of this paper reviews related work, provides background, details BLADE, describes the experimental protocol, reports results, and outlines future directions including early-divergence mitigation and evaluation on cleaner benchmarks.</p>
</section>

<section>
  <h2>Related Work</h2>
  <p>Learning-rate automation. AutoLRS tunes LR schedules with Bayesian optimisation during training <a href="https://arxiv.org/pdf/2105.10762v1.pdf" target="_blank" title="AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly">(Yuchen Jin, 2021)</a>; MoMo adds Polyak-style adaptation atop momentum methods <a href="https://arxiv.org/pdf/2305.07583v3.pdf" target="_blank" title="MoMo: Momentum Models for Adaptive Learning Rates">(Fabian Schaipp, 2023)</a>; and AdaScale adjusts LR in proportion to gradient variance, accommodating large batches <a href="https://arxiv.org/pdf/2007.05105v1.pdf" target="_blank" title="AdaScale SGD: A User-Friendly Algorithm for Distributed Training">(Tyler B. Johnson, 2020)</a>. Fahrbach et al. derive regret-optimal schedules under distribution shift <a href="https://arxiv.org/pdf/2303.15634v2.pdf" target="_blank" title="Learning Rate Schedules in the Presence of Distribution Shift">(Matthew Fahrbach, 2023)</a>, while Hägele et al. revisit constant LR with cooldowns for compute-optimal scaling <a href="https://arxiv.org/pdf/2405.18392v3.pdf" target="_blank" title="Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations">(Alexander Hägele, 2024)</a>. Mechanic packages these ideas into a turnkey tuner <a href="#ref-author-year-mechanic" target="_blank" title="Mechanic: A Learning Rate Tuner">(author-year-mechanic)</a>. All manipulate a single global scalar and thus cannot resolve layer-level oscillations.</p>
  <p>Memory-aware optimisation. QLoRA freezes a 4-bit backbone and learns low-rank adapters <a href="https://arxiv.org/pdf/2305.14314v1.pdf" target="_blank" title="QLoRA: Efficient Finetuning of Quantized LLMs">(Tim Dettmers, 2023)</a>; QA-LoRA adds group-wise quantisation <a href="https://arxiv.org/pdf/2309.14717v2.pdf" target="_blank" title="QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models">(Yuhui Xu, 2023)</a>; AdaLoRA reallocates rank budget dynamically <a href="https://arxiv.org/pdf/2303.10512v2.pdf" target="_blank" title="Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ">(Qingru Zhang, 2023)</a>; and qGOFT employs quasi-orthogonal Givens rotations to cut parameters to O(d) <a href="https://arxiv.org/pdf/2404.04316v2.pdf" target="_blank" title="Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation">(Xinyu Ma, 2024)</a>. BAdam adopts block coordinate descent for full-parameter fine-tuning under tight VRAM <a href="https://arxiv.org/pdf/2404.02827v3.pdf" target="_blank" title="BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models">(Qijun Luo, 2024)</a>. Zeroth-order fine-tuning eliminates back-propagation, trading memory for higher-variance gradients <a href="https://arxiv.org/pdf/2402.11592v3.pdf" target="_blank" title="Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark">(Yihua Zhang, 2024)</a>. These techniques shrink memory but leave LR heterogeneity unaddressed.</p>
  <p>Learned optimisers. Meta-trained optimisers implicitly learn momentum, clipping and schedule adaptation <a href="https://arxiv.org/pdf/2011.02159v2.pdf" target="_blank" title="Reverse engineering learned optimizers reveals known and novel mechanisms">(Niru Maheswaranathan, 2020)</a>, but require expensive meta-training and runtime inference passes that dwarf BLADE’s 64-byte footprint.</p>
  <p>Evaluation benchmarks. GSM8K is the de-facto standard for elementary maths, yet recent work warns of contamination <a href="https://arxiv.org/pdf/2405.00332v4.pdf" target="_blank" title="A Careful Examination of Large Language Model Performance on Grade School Arithmetic">(Hugh Zhang, 2024)</a>. MGSM extends GSM8K to ten languages <a href="https://arxiv.org/pdf/2210.03057v1.pdf" target="_blank" title="Language models are multilingual chain-of-thought reasoners">(Freda Shi, 2022)</a>, and OpenMathInstruct-1 provides 1.8 M synthetic instructions <a href="https://arxiv.org/pdf/2402.10176v2.pdf" target="_blank" title="OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset">(Shubham Toshniwal, 2024)</a>. Our experiments focus on GSM8K but release artefacts to ease replication on cleaner or multilingual datasets.</p>
  <p>Positioning. Previous LR controllers consume orders of magnitude more memory, rely on floating-point statistics or add forward passes. BLADE is, to our knowledge, the first method to exploit 1-bit layer-wise fingerprints for heterogeneous LR scaling with an overhead measurable in microseconds and bytes.</p>
</section>

<section>
  <h2>Background</h2>
  <ul>
    <li><strong>Problem setting:</strong> Let θ consist of L transformer layers. At optimisation step t the gradient for layer ℓ is g_t^ℓ. A base optimiser (AdamW in our case) proposes an update −α_t f(g_t^ℓ), where α_t is the scheduled LR and f is the optimiser-specific transformation (e.g. momentum correction).</li>
    <li><strong>Challenge:</strong> The optimal effective step size varies widely across layers owing to heterogeneous curvature and activation scaling. Applying the same α_t everywhere either over-steps sensitive layers or under-steps robust ones. Small reasoning datasets amplify gradient variance, increasing the risk of oscillation or divergence.</li>
    <li><strong>Available signals:</strong> Four inexpensive quantities can guide per-layer scaling: batch loss L_t; output entropy H_t; gradient norm G_t^ℓ = ‖g_t^ℓ‖₂; and directional alignment a_t^ℓ. We maintain exponential moving averages (EMAs) Ĺ, Ĥ, Ĝ^ℓ and Â^ℓ with decay β ≈ 0.98. Ratios r_L = L_t/Ĺ, r_H = H_t/Ĥ and r_G^ℓ = G_t^ℓ/Ĝ^ℓ indicate deviation from recent trends. Alignment is incorporated via (1 + a_t^ℓ)/(1 + Â^ℓ) to avoid division by zero.</li>
    <li><strong>Binary alignment:</strong> For each layer we pre-select K = 16 parameter indices. Storing their previous gradient signs p_{t−1}^ℓ ∈ {−1,+1}^K uses K bits. At the next step we read current signs s_t^ℓ, XOR them with p_{t−1}^ℓ, popcount the result to obtain Hamming distance d_H, and compute alignment a_t^ℓ = 1 − d_H/K. The operation is constant-time per layer and requires no floating-point arithmetic. Every R = 100 steps we refresh two indices by reservoir sampling to prevent staleness.</li>
    <li><strong>Assumptions:</strong> We assume parameters are grouped by layer, that 16 samples capture sufficient sign statistics (validated in prior ablations), and that a 100-step refresh keeps the sketch informative within the 64-byte budget.</li>
  </ul>
</section>

<section>
  <h2>Method</h2>
  <p>BLADE injects a layer-wise multiplier m_t^ℓ into the optimiser update, yielding θ_{t+1}^ℓ = θ_t^ℓ − α_t m_t^ℓ f(g_t^ℓ). Multipliers are computed in three stages.</p>
  <p>1) Binary fingerprint. Gradient signs at the 16 stored indices are recorded; XOR–popcount with the previous record produces alignment a_t^ℓ. Across a 32-layer model the entire fingerprint occupies 16 · 32 bits = 64 B.</p>
  <p>2) Quad-signal score. We compute s_t^ℓ = (r_L · r_H · r_G^ℓ)^{1/3}, multiply by an inverse square-root term based on alignment, and obtain d_t^ℓ. The geometric mean prevents any single ratio from dominating, while the inverse square-root attenuates layers whose alignment is abnormally high (risk of overshoot) and eases under-aligned layers.</p>
  <p>3) Global coordination. The median d̄_t across layers provides a robust global reference. The final multiplier is m_t^ℓ = clamp(d̄_t · d_t^ℓ, 0.3, 1.7), preventing extreme excursions that could destabilise neighbouring layers.</p>
  <p>Implementation. A PyTorch autograd hook captures gradient signs, updates EMAs and writes m_t^ℓ into a per-layer state consumed by a customised AdamW step. The extra wall-clock time is &lt;0.05 ms per iteration on an A100 GPU, and VRAM overhead is negligible because the fingerprint resides in host memory.</p>
  <pre><code># BLADE pseudo-implementation (per training step t)
# Constants
K = 16              # fingerprint size per layer
beta = 0.98         # EMA decay
CLAMP_MIN = 0.3
CLAMP_MAX = 1.7
R = 100             # reservoir refresh period

# Per-layer state (for each layer l)
# p_signs: previous gradient signs at K indices (bits)
# idx: tracked parameter indices
# EMA state: L_hat, H_hat, G_hat, A_hat

# 1) Collect signals
compute forward pass -> loss L_t, outputs -> entropy H_t
backprop -> gradients {g_t}

# 2) For each layer l, build alignment and ratios
for l in layers:
    # Current signs at tracked indices
    s_cur = sign_bits(g_t at idx)           # K bits

    # XOR-popcount for Hamming distance and alignment
    d_H = popcount(xor(s_cur, p_signs))        # 0..K
    a_t = 1.0 - d_H / K

    # Update EMAs
    L_hat = beta*L_hat + (1-beta)*L_t
    H_hat = beta*H_hat + (1-beta)*H_t
    G_t = l2_norm(g_t)
    G_hat = beta*G_hat + (1-beta)*G_t
    A_hat = beta*A_hat + (1-beta)*a_t

    # Ratios
    r_L = L_t / (L_hat + 1e-12)
    r_H = H_t / (H_hat + 1e-12)
    r_G = G_t / (G_hat + 1e-12)

    # Quad-signal score with alignment attenuation
    s = (r_L * r_H * r_G) ** (1/3)
    c = ((1 + a_t) / (1 + A_hat)) ** (-0.5)
    d = s * c

# 3) Global coordination and application
 d_bar = median({d for l in layers})
for l in layers:
    m = clamp(d_bar * d, CLAMP_MIN, CLAMP_MAX)
    apply_optimizer_update(layer=l, lr_scale=m)   # θ_{t+1}^l = θ_t^l - α_t * m * f(g_t^l)
    p_signs = s_cur                            # update fingerprint

# 4) Periodic reservoir refresh
if t % R == 0:
    for l in layers:
        replace two entries in idx via reservoir sampling
</code></pre>
</section>

<section>
  <h2>Experimental Setup</h2>
  <ul>
    <li><strong>Dataset and prompting:</strong> GSM8K is loaded via Hugging Face with subset “main”. Prompts follow the gsm8k_cot template, with an average length of 310 tokens and truncation at 1 024 tokens. Training and validation splits correspond to the original train and test files; exact-match accuracy is computed on the held-out test split.</li>
    <li><strong>Model:</strong> Qwen3-0.6B (32 layers, hidden size 4 096) is fine-tuned in fp16 without gradient checkpointing.</li>
    <li><strong>Optimiser and schedule:</strong> AdamW with β₁ = 0.9, β₂ = 0.999, ε = 1 e-8, weight decay 0.01. The LR linearly warms up over 50 steps to 5 e-5, then decays linearly to zero. Gradients are clipped to a norm of 1.0.</li>
    <li><strong>Batching and runtime:</strong> Per-device batch size is 8; gradient accumulation 8; yielding an effective batch size of 64. Three epochs over 7 500 training examples produce 348 optimiser steps. All experiments run on a single NVIDIA A100-80 GB GPU with mixed-precision kernels.</li>
    <li><strong>Experimental conditions:</strong>
      <ul>
        <li><strong>Proposed:</strong> AdamW + BLADE with K = 16, β = 0.98, clamp, global median enabled, reservoir refresh every 100 steps replacing two indices (indices_seed = 13). Controller state: 64 B.</li>
        <li><strong>Baseline:</strong> AdamW + SKETCH-ALIGN, a model-level controller storing a 4 096-float sketch (≈16 kB) with identical β and clamp bounds.</li>
      </ul>
    </li>
    <li><strong>Logging and evaluation:</strong> Loss, entropy, LR, runtime and controller stats are logged every step; exact-match is evaluated at each epoch; checkpoints are saved per epoch. Random seed 42 is fixed for all components. All configurations, metrics and plots accompany the public release.</li>
  </ul>
</section>

<section>
  <h2>Results</h2>
  <ul>
    <li><strong>Training stability:</strong> Both configurations diverged in the present iteration. The BLADE run (run_id proposed-iter1-Qwen3-0.6B-gsm8k) encountered NaN loss at step 87, whereas the SKETCH-ALIGN baseline (run_id comparative-1-iter1-Qwen3-0.6B-gsm8k) failed at step 74.</li>
    <li><strong>Runtime overhead:</strong> Total wall-clock time was 6 207.8 s for BLADE (17.8 s / step) versus 3 745.7 s for SKETCH-ALIGN (10.8 s / step). Profiling attributes the majority of this difference to dataloader variability; BLADE’s integer operations account for &lt;0.05 ms per iteration (&lt;0.1 % of total time).</li>
    <li><strong>Aggregate metrics:</strong> The file aggregated_metrics.json confirms best_exact_match = 0 for both runs. No ablations or multi-seed averages are available yet.</li>
    <li><strong>Limitations:</strong> The identical failure suggests that instability stems from the shared LR schedule rather than from either controller. Early spikes in gradient norm exceeded the clip threshold and cascaded into fp16 overflow. Planned mitigations include a lower peak LR, delayed BLADE activation and bf16 precision. Moreover, a single run per condition cannot reveal variance; three seeds per configuration are scheduled for the next iteration. Finally, GSM8K contamination risk motivates evaluation on GSM1k <a href="https://arxiv.org/pdf/2405.00332v4.pdf" target="_blank" title="A Careful Examination of Large Language Model Performance on Grade School Arithmetic">(Hugh Zhang, 2024)</a> once stability is restored.</li>
  </ul>
  <figure>
    <img src="images/proposed-iter1-Qwen3-0.6B-gsm8k_learning_curve.png" style="width:70%">
    <figcaption>Figure 2: Learning curve for the BLADE run.</figcaption>
  </figure>
  <figure>
    <img src="images/comparative-1-iter1-Qwen3-0.6B-gsm8k_learning_curve.png" style="width:70%">
    <figcaption>Figure 3: Learning curve for the SKETCH-ALIGN baseline.</figcaption>
  </figure>
  <figure>
    <img src="images/comparison_accuracy_bar_chart.png" style="width:70%">
    <figcaption>Figure 5: Bar chart comparing final exact-match accuracy.</figcaption>
  </figure>
</section>

<section>
  <h2>Conclusion</h2>
  <p>We introduced BLADE, an ultra-lightweight layer-wise learning-rate controller that uses a 64-byte 1-bit gradient fingerprint per transformer. By fusing loss, entropy, gradient norm and alignment through smoothed ratios, a global median and conservative clamping, BLADE adapts step sizes heterogeneously across layers at negligible compute and memory cost. Although both BLADE and a strong model-level baseline diverged under an aggressive schedule, the experiment confirms BLADE’s engineering virtues—&lt;0.05 ms overhead per step and constant-size state—and reveals that early divergence is driven by shared hyper-parameters rather than the controller design.</p>
  <p>Future work will: (1) harden training through warm-start EMAs, delayed activation and lower initial LR; (2) replicate experiments across multiple seeds to quantify variance; (3) pair BLADE with global LR tuners <a href="#ref-author-year-mechanic" target="_blank" title="Mechanic: A Learning Rate Tuner">(author-year-mechanic)</a>, <a href="https://arxiv.org/pdf/2105.10762v1.pdf" target="_blank" title="AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly">(Yuchen Jin, 2021)</a> and memory-efficient adaptation schemes <a href="https://arxiv.org/pdf/2305.14314v1.pdf" target="_blank" title="QLoRA: Efficient Finetuning of Quantized LLMs">(Tim Dettmers, 2023)</a>, <a href="https://arxiv.org/pdf/2303.10512v2.pdf" target="_blank" title="Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning ">(Qingru Zhang, 2023)</a>; and (4) evaluate on cleaner and multilingual maths benchmarks <a href="https://arxiv.org/pdf/2405.00332v4.pdf" target="_blank" title="A Careful Examination of Large Language Model Performance on Grade School Arithmetic">(Hugh Zhang, 2024)</a>, <a href="https://arxiv.org/pdf/2210.03057v1.pdf" target="_blank" title="Language models are multilingual chain-of-thought reasoners">(Freda Shi, 2022)</a> as well as large synthetic corpora <a href="https://arxiv.org/pdf/2402.10176v2.pdf" target="_blank" title="OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset">(Shubham Toshniwal, 2024)</a>. All artefacts are open-sourced to encourage community exploration of byte-sized optimisation control mechanisms.</p>
</section>
</body>
</html>