{
  "research_topic": "Learning rate optimization for fine-tuning Qwen3-0.6B on GSM8K elementary math problems",
  "queries": [
    "learning rate optimization",
    "fine-tuning Qwen3-0.6B",
    "GSM8K elementary math",
    "adaptive lr scheduling",
    "LLM optimizer configurations"
  ],
  "research_study_list": [
    {
      "title": "AutoLRS: Automatic Learning-Rate Schedule by Bayesian Optimization on the Fly"
    },
    {
      "title": "Reverse engineering learned optimizers reveals known and novel mechanisms"
    },
    {
      "title": "Mechanic: A Learning Rate Tuner"
    },
    {
      "title": "MoMo: Momentum Models for Adaptive Learning Rates"
    },
    {
      "title": "Where Do Large Learning Rates Lead Us?"
    },
    {
      "title": "QA-LoRA: Quantization-Aware Low-Rank Adaptation of Large Language Models"
    },
    {
      "title": "QLoRA: Efficient Finetuning of Quantized LLMs"
    },
    {
      "title": "Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning "
    },
    {
      "title": "Parameter Efficient Quasi-Orthogonal Fine-Tuning via Givens Rotation"
    },
    {
      "title": "A Careful Examination of Large Language Model Performance on Grade School Arithmetic"
    },
    {
      "title": "Language models are multilingual chain-of-thought reasoners"
    },
    {
      "title": "OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset"
    },
    {
      "title": "Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads"
    },
    {
      "title": "Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations"
    },
    {
      "title": "Learning Rate Schedules in the Presence of Distribution Shift"
    },
    {
      "title": "AdaScale SGD: A User-Friendly Algorithm for Distributed Training"
    },
    {
      "title": "Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark"
    },
    {
      "title": "GPTSwarm: Language Agents as Optimizable Graphs"
    },
    {
      "title": "BAdam: A Memory Efficient Full Parameter Optimization Method for Large Language Models"
    }
  ]
}